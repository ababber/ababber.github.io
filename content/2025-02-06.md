---
title: 2025-02-06
draft: true 
permalink: https://www.ankitbabber.com/2025-02-06
tags:
  - llm
  - build
  - macOS
  - brew
  - vulkan
  - amd
  - intel 
---

## Why setup a `Vulkan` backend for [llama.cpp](https://github.com/ggerganov/llama.cpp) on macOS with an AMD eGPU

> If you have an Intel Mac, then you know that the CPU and integrated GPU are insufficient for running an LLM locally. The `Metal` backend build [llama.cpp](https://github.com/ggerganov/llama.cpp) only targets the CPU and Apple GPUs. So, if you have an Intel Mac with an eGPU, then I recommend setting up the `Vulkan` backend as per the instructions on [llama.cpp](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md#vulkan) to improve performance of a LLM running locally.

## Start simple

1. [Download and install Vulkan SDK]()